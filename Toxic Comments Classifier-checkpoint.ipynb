{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data source: \n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "### Objectives:\n",
    "Build a multi-headed model capable of detecting and assigning probabilities for different types of of toxicity such as: \n",
    "1. toxic\n",
    "2. severe toxic\n",
    "3. obscene\n",
    "4. threat\n",
    "5. insult\n",
    "6. identity hate\n",
    "\n",
    "The dataset comprises of comments from Wikipedia's talk page (ie. discussion) pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCreate a new environment\\n\\n1. Open Anaconda Prompt\\n2. Enter 'conda env create -f toxic_test.yml'\\n3. Enter 'python -m ipykernel install --user --name toxic --display-name toxic\\n\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Create a new environment\n",
    "\n",
    "1. Open Anaconda Prompt\n",
    "2. Enter 'conda env create -f toxic_test.yml'\n",
    "3. Enter 'python -m ipykernel install --user --name toxic --display-name toxic\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import relevant libraries and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\prathi\\anaconda3\\lib\\site-packages (3.6.5)\n",
      "Requirement already satisfied: click in c:\\users\\prathi\\anaconda3\\lib\\site-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\prathi\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\prathi\\anaconda3\\lib\\site-packages (from nltk) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\prathi\\anaconda3\\lib\\site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\prathi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "# Install the nltk library in case you have not yet installed it\n",
    "# pip stands for preferred installer program under Python Package Index (PyPI)\n",
    "\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant packages\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import re \n",
    "import pandas as pd\n",
    "import string\n",
    "import seaborn as sns\n",
    "\n",
    "from nltk.corpus import stopwords  # Remove useless words\n",
    "from nltk.stem.lancaster import LancasterStemmer  # Convert words to base form; aggressive\n",
    "\n",
    "# Import packages that help us to create document-term matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In case there is a bug, just download stopwords again.\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stopwords are a list of 'useless' words\n",
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the CSV and take a peek at 1st 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entries with all 0 under the 6 categories are neutral and considered as non-toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A clean dataset with no missing values. Sweet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how the 1st 3 comments look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\""
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\nMore\\nI can\\'t make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['comment_text'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The comments are dirty with '\\n' values, symbols, numbers that will not be useful for analysis. We will be removing these further down the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.904156\n",
       "1    0.095844\n",
       "Name: toxic, dtype: float64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check percentage of comments that are toxic compared to normal comments\n",
    "data.toxic.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. It seems that 90% of the comments fall in the toxic category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new subset of the data by only taking the 2nd column onwards (comments and categories)\n",
    "data_count=data.iloc[:,2:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            15294\n",
       "severe_toxic      1595\n",
       "obscene           8449\n",
       "threat             478\n",
       "insult            7877\n",
       "identity_hate     1405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159571\n"
     ]
    }
   ],
   "source": [
    "num_rows = len(data)\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edits made under my username hardcore metallica fan were reverted  they weren t vandalisms  just closure on some gas after i voted at new york dolls fac  and please don t remove the template from the talk page since i m retired now        '"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text preprocessing steps - remove numbers, capital letters, punctuation, '\\n'\n",
    "import re\n",
    "import string\n",
    "\n",
    "# remove all numbers with letters attached to them\n",
    "alphanumeric = lambda x: re.sub('\\w*\\d\\w*', ' ', x)\n",
    "\n",
    "# '[%s]' % re.escape(string.punctuation),' ' - replace punctuation with white space\n",
    "# .lower() - convert all strings to lowercase \n",
    "punc_lower = lambda x: re.sub('[%s]' % re.escape(string.punctuation), ' ', x.lower())\n",
    "\n",
    "# Remove all '\\n' in the string and replace it with a space\n",
    "remove_n = lambda x: re.sub(\"\\n\", \" \", x)\n",
    "\n",
    "# Remove all non-ascii characters \n",
    "remove_non_ascii = lambda x: re.sub(r'[^\\x00-\\x7f]',r' ', x)\n",
    "\n",
    "# Apply all the lambda functions wrote previously through .map on the comments column\n",
    "data['comment_text'] = data['comment_text'].map(alphanumeric).map(punc_lower).map(remove_n).map(remove_non_ascii)\n",
    "\n",
    "data['comment_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate our dataset into 6 sections. Each section is comment + 1 category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox = data.loc[:,['id','comment_text','toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edits made under my username hardcore metallica fan were reverted  they weren t vandalisms  just closure on some gas after i voted at new york dolls fac  and please don t remove the template from the talk page since i m retired now        '"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tox['comment_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d aww  he matches this background colour i m seemingly stuck with  thanks    talk       january       utc '"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tox['comment_text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sev = data.loc[:,['id','comment_text','severe_toxic']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_obs = data.loc[:,['id','comment_text','obscene']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_thr = data.loc[:,['id','comment_text','threat']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ins = data.loc[:,['id','comment_text','insult']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ide = data.loc[:,['id','comment_text','identity_hate']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Earlier, we saw that comments that are toxic (and other forms of toxicity) make up less than 10% of the comments in the data. This leads to the issue of class imbalance.\n",
    "\n",
    "#### We can deal with class imbalance by taking a subset of the data where the proportion of the toxic comments are at least 20% (ideally 50%) in relation to non-toxic comments.\n",
    "\n",
    "#### For a start, we can take 5000 rows of comments that are toxic and concatenate them row-wise with those that are not toxic so that we have a balanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>d aww  he matches this background colour i m s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man  i m really not trying to edit war  it...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>more i can t make any real suggestions on im...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>you  sir  are my hero  any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic\n",
       "0  0000997932d777bf  explanation why the edits made under my userna...      0\n",
       "1  000103f0d9cfb60f  d aww  he matches this background colour i m s...      0\n",
       "2  000113f07ec002fd  hey man  i m really not trying to edit war  it...      0\n",
       "3  0001b41b1c6bb37e    more i can t make any real suggestions on im...      0\n",
       "4  0001d958c54c6e35  you  sir  are my hero  any chance you remember...      0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tox.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tox_1 = data_tox[data_tox['toxic'] == 1].iloc[0:5000,:]\n",
    "data_tox_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tox_0 = data_tox[data_tox['toxic'] == 0].iloc[0:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tox_done = pd.concat([data_tox_1, data_tox_0], axis=0)\n",
    "data_tox_done.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all categories have 5000 rows. So we should count them first and make them balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              1595\n",
       "comment_text    1595\n",
       "severe_toxic    1595\n",
       "dtype: int64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sev[data_sev['severe_toxic'] == 1].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3190, 3)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sev_1 = data_sev[data_sev['severe_toxic'] == 1].iloc[0:1595,:]\n",
    "data_sev_0 = data_sev[data_sev['severe_toxic'] == 0].iloc[0:1595,:]\n",
    "data_sev_done = pd.concat([data_sev_1, data_sev_0], axis=0)\n",
    "data_sev_done.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. We only had 1,595 comments that are severely toxic. We combine it together with another 1,595 comments that are not toxic to form a new dataset that is balanced. We repeat this for all other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              8449\n",
       "comment_text    8449\n",
       "obscene         8449\n",
       "dtype: int64"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obs[data_obs['obscene'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_obs_1 = data_obs[data_obs['obscene'] == 1].iloc[0:5000,:]\n",
    "data_obs_0 = data_obs[data_obs['obscene'] == 0].iloc[0:5000,:]\n",
    "data_obs_done = pd.concat([data_obs_1, data_obs_0], axis=0)\n",
    "data_obs_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              478\n",
       "comment_text    478\n",
       "threat          478\n",
       "dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_thr[data_thr['threat'] == 1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of threat comments of 478 is too miniscule when addded with another 478 clean comments for a proper analysis. We decided that the clean comments will comprise 80% at the most of the dataset, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390, 3)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_thr_1 = data_thr[data_thr['threat'] == 1].iloc[0:478,:]\n",
    "\n",
    "# We include 1912 comments that have no threat so that the data with threat (478) will represent 20% of the dataset.\n",
    "data_thr_0 = data_thr[data_thr['threat'] == 0].iloc[0:1912,:]  \n",
    "data_thr_done = pd.concat([data_thr_1, data_thr_0], axis=0)\n",
    "data_thr_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              7877\n",
       "comment_text    7877\n",
       "insult          7877\n",
       "dtype: int64"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ins[data_ins['insult'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ins_1 = data_ins[data_ins['insult'] == 1].iloc[0:5000,:]\n",
    "data_ins_0 = data_ins[data_ins['insult'] == 0].iloc[0:5000,:]\n",
    "data_ins_done = pd.concat([data_ins_1, data_ins_0], axis=0)\n",
    "data_ins_done.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               1405\n",
       "comment_text     1405\n",
       "identity_hate    1405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ide[data_ide['identity_hate'] == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7025, 3)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ide_1 = data_ide[data_ide['identity_hate'] == 1].iloc[0:1405,:] # 20%\n",
    "data_ide_0 = data_ide[data_ide['identity_hate'] == 0].iloc[0:5620,:] # 80%\n",
    "data_ide_done = pd.concat([data_ide_1, data_ide_0], axis=0)\n",
    "data_ide_done.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder: Number of comments that fall into the following categories:\n",
    "\n",
    "- Toxic (14,000+) \n",
    "- Severe Toxic (1595)\n",
    "- Obscene (8449)\n",
    "- Threat (478)\n",
    "- Insult (7877)\n",
    "- Identity Hate (1405)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### df_****_done refers to the dataframes of each class that has been balanced (at least 20/80 proportion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                      | DF name       | No. of pts (1) | No. of pts (0) | Total data pts |\n",
    "|----------------------|---------------|----------------|----------------|----------------|\n",
    "| Toxic                | data_tox_done | 5000           | 5000           | 10000          |\n",
    "| Severe Toxic         | data_sev_done | 1595           | 1595           | 3190           |\n",
    "| Obscene (8449)       | data_obs_done | 5000           | 5000           | 10000          |\n",
    "| Threat (478)         | data_thr_done | 478            | 1912           | 2390           |\n",
    "| Insult (7877)        | data_ins_done | 5000           | 5000           | 10000          |\n",
    "| Identity Hate (1405) | data_ide_done | 1405           | 5620           | 7025           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant packages for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for pre-processing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Import tools to split data and evaluate model performance\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_curve, fbeta_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "# Import ML algos\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create simple function that takes in a dataset and allows user to choose dataset, toxicity label, vectorizer and number of ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "df_done: data_tox_done, data_sev_done, ...\n",
    "label: toxic, severe_toxic, ...\n",
    "vectorizer values: CountVectorizer, TfidfVectorizer\n",
    "gram_range values: (1,1) for unigram, (2,2) for bigram\n",
    "'''\n",
    "def cv_tf_train_test(df_done,label,vectorizer,ngram):\n",
    "\n",
    "    ''' Train/Test split'''\n",
    "    # Split the data into X and y data sets\n",
    "    X = df_done.comment_text\n",
    "    y = df_done[label]\n",
    "\n",
    "    # Split our data into training and test data \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    ''' Count Vectorizer/TF-IDF '''\n",
    "\n",
    "    # Create a Vectorizer object and remove stopwords from the table\n",
    "    cv1 = vectorizer(ngram_range=(ngram), stop_words='english')\n",
    "    \n",
    "    X_train_cv1 = cv1.fit_transform(X_train) # Learn the vocabulary dictionary and return term-document matrix\n",
    "    X_test_cv1  = cv1.transform(X_test)      # Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "    \n",
    "    # Output a Dataframe of the CountVectorizer with unique words as the labels\n",
    "    # test = pd.DataFrame(X_train_cv1.toarray(), columns=cv1.get_feature_names())\n",
    "        \n",
    "    ''' Initialize all model objects and fit the models on the training data '''\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X_train_cv1, y_train)\n",
    "    print('lr done')\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn.fit(X_train_cv1, y_train)\n",
    "\n",
    "    bnb = BernoulliNB()\n",
    "    bnb.fit(X_train_cv1, y_train)\n",
    "    print('bnb done')\n",
    "    \n",
    "    mnb = MultinomialNB()\n",
    "    mnb.fit(X_train_cv1, y_train)\n",
    "    print('mnb done')\n",
    "    \n",
    "    svm_model = LinearSVC()\n",
    "    svm_model.fit(X_train_cv1, y_train)\n",
    "\n",
    "    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    randomforest.fit(X_train_cv1, y_train)\n",
    "    print('rdf done')\n",
    "    \n",
    "    # Create a list of F1 score of all models \n",
    "    f1_score_data = {'F1 Score':[f1_score(lr.predict(X_test_cv1), y_test), f1_score(knn.predict(X_test_cv1), y_test), \n",
    "                                f1_score(bnb.predict(X_test_cv1), y_test), f1_score(mnb.predict(X_test_cv1), y_test),\n",
    "                                f1_score(svm_model.predict(X_test_cv1), y_test), f1_score(randomforest.predict(X_test_cv1), y_test)]} \n",
    "                          \n",
    "    # Create DataFrame with the model names as column labels\n",
    "    df_f1 = pd.DataFrame(f1_score_data, index=['Log Regression','KNN', 'BernoulliNB', 'MultinomialNB', 'SVM', 'Random Forest'])  \n",
    "\n",
    "    return df_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a TF-IDF vectorizer object for each category and calculate the F1 scores across all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr done\n",
      "bnb done\n",
      "mnb done\n",
      "rdf done\n",
      "Time taken: 25.121816158294678 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(toxic)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.861234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.185120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.776521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.874958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.876133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.838055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(toxic)\n",
       "Log Regression         0.861234\n",
       "KNN                    0.185120\n",
       "BernoulliNB            0.776521\n",
       "MultinomialNB          0.874958\n",
       "SVM                    0.876133\n",
       "Random Forest          0.838055"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def cv_tf_train_test(df_done,label,vectorizer,ngram)\n",
    "vectorizer values: CountVectorizer, TfidfVectorizer\n",
    "ngram_range values: (1,1) for unigram, (2,2) for bigram\n",
    "'''\n",
    "\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_tox_cv = cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))\n",
    "df_tox_cv.rename(columns={'F1 Score': 'F1 Score(toxic)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_tox_cv\n",
    "\n",
    "# Various permutations of the dataset, category, vectorizer and n-gram\n",
    "\n",
    "# cv_tf_train_test(data_tox_done, 'toxic', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_sev_done, 'severe_toxic', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_obs_done, 'obscene', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_thr_done, 'threat', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ins_done, 'insult', CountVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ide_done, 'identity_hate', CountVectorizer, (1,1))\n",
    "\n",
    "# cv_tf_train_test(data_tox_done, 'toxic', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))\n",
    "# cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr done\n",
      "bnb done\n",
      "mnb done\n",
      "rdf done\n",
      "Time taken: 4.36896538734436 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(severe_toxic)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.927879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.857416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.803707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.936170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.926004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.934874</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(severe_toxic)\n",
       "Log Regression                0.927879\n",
       "KNN                           0.857416\n",
       "BernoulliNB                   0.803707\n",
       "MultinomialNB                 0.936170\n",
       "SVM                           0.926004\n",
       "Random Forest                 0.934874"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_sev_cv = cv_tf_train_test(data_sev_done, 'severe_toxic', TfidfVectorizer, (1,1))\n",
    "df_sev_cv.rename(columns={'F1 Score': 'F1 Score(severe_toxic)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "df_sev_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr done\n",
      "bnb done\n",
      "mnb done\n",
      "rdf done\n",
      "Time taken: 23.51699447631836 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(obscene)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.908655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.519056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.787830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.901463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.921378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(obscene)\n",
       "Log Regression           0.908655\n",
       "KNN                      0.519056\n",
       "BernoulliNB              0.787830\n",
       "MultinomialNB            0.901463\n",
       "SVM                      0.921378\n",
       "Random Forest            0.909091"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_obs_cv = cv_tf_train_test(data_obs_done, 'obscene', TfidfVectorizer, (1,1))\n",
    "df_obs_cv.rename(columns={'F1 Score': 'F1 Score(obscene)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_obs_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr done\n",
      "bnb done\n",
      "mnb done\n",
      "rdf done\n",
      "Time taken: 2.974821090698242 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(threat)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.628821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.311828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.504762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.786765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.795539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(threat)\n",
       "Log Regression          0.628821\n",
       "KNN                     0.720000\n",
       "BernoulliNB             0.311828\n",
       "MultinomialNB           0.504762\n",
       "SVM                     0.786765\n",
       "Random Forest           0.795539"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_thr_cv = cv_tf_train_test(data_thr_done, 'threat', TfidfVectorizer, (1,1))\n",
    "df_thr_cv.rename(columns={'F1 Score': 'F1 Score(threat)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_thr_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr done\n",
      "bnb done\n",
      "mnb done\n",
      "rdf done\n",
      "Time taken: 22.95567226409912 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(insult)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.896599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.257992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.783762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.897411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.902619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.883993</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(insult)\n",
       "Log Regression          0.896599\n",
       "KNN                     0.257992\n",
       "BernoulliNB             0.783762\n",
       "MultinomialNB           0.897411\n",
       "SVM                     0.902619\n",
       "Random Forest           0.883993"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_ins_cv = cv_tf_train_test(data_ins_done, 'insult', TfidfVectorizer, (1,1))\n",
    "df_ins_cv.rename(columns={'F1 Score': 'F1 Score(insult)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_ins_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr done\n",
      "bnb done\n",
      "mnb done\n",
      "rdf done\n",
      "Time taken: 12.424582242965698 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(identity_hate)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.699029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.230159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.549206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.485857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.797516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.768448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(identity_hate)\n",
       "Log Regression                 0.699029\n",
       "KNN                            0.230159\n",
       "BernoulliNB                    0.549206\n",
       "MultinomialNB                  0.485857\n",
       "SVM                            0.797516\n",
       "Random Forest                  0.768448"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "t0 = time.time()\n",
    "\n",
    "df_ide_cv = cv_tf_train_test(data_ide_done, 'identity_hate', TfidfVectorizer, (1,1))\n",
    "df_ide_cv.rename(columns={'F1 Score': 'F1 Score(identity_hate)'}, inplace=True)\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = 'Time taken: {} seconds'.format(t1-t0)\n",
    "print(total)\n",
    "\n",
    "df_ide_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1 Score(toxic)</th>\n",
       "      <th>F1 Score(severe_toxic)</th>\n",
       "      <th>F1 Score(obscene)</th>\n",
       "      <th>F1 Score(insult)</th>\n",
       "      <th>F1 Score(threat)</th>\n",
       "      <th>F1 Score(identity_hate)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Log Regression</th>\n",
       "      <td>0.861234</td>\n",
       "      <td>0.927879</td>\n",
       "      <td>0.908655</td>\n",
       "      <td>0.896599</td>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.699029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.185120</td>\n",
       "      <td>0.857416</td>\n",
       "      <td>0.519056</td>\n",
       "      <td>0.257992</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.230159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BernoulliNB</th>\n",
       "      <td>0.776521</td>\n",
       "      <td>0.803707</td>\n",
       "      <td>0.787830</td>\n",
       "      <td>0.783762</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.549206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.874958</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.901463</td>\n",
       "      <td>0.897411</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.485857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.876133</td>\n",
       "      <td>0.926004</td>\n",
       "      <td>0.921378</td>\n",
       "      <td>0.902619</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.797516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.838055</td>\n",
       "      <td>0.934874</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.883993</td>\n",
       "      <td>0.795539</td>\n",
       "      <td>0.768448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                F1 Score(toxic)  F1 Score(severe_toxic)  F1 Score(obscene)  \\\n",
       "Log Regression         0.861234                0.927879           0.908655   \n",
       "KNN                    0.185120                0.857416           0.519056   \n",
       "BernoulliNB            0.776521                0.803707           0.787830   \n",
       "MultinomialNB          0.874958                0.936170           0.901463   \n",
       "SVM                    0.876133                0.926004           0.921378   \n",
       "Random Forest          0.838055                0.934874           0.909091   \n",
       "\n",
       "                F1 Score(insult)  F1 Score(threat)  F1 Score(identity_hate)  \n",
       "Log Regression          0.896599          0.628821                 0.699029  \n",
       "KNN                     0.257992          0.720000                 0.230159  \n",
       "BernoulliNB             0.783762          0.311828                 0.549206  \n",
       "MultinomialNB           0.897411          0.504762                 0.485857  \n",
       "SVM                     0.902619          0.786765                 0.797516  \n",
       "Random Forest           0.883993          0.795539                 0.768448  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's combine the dataframes into a master dataframe to compare F1 scores across all categories.\n",
    "f1_all = pd.concat([df_tox_cv, df_sev_cv, df_obs_cv, df_ins_cv, df_thr_cv, df_ide_cv], axis=1)\n",
    "f1_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose the combined F1 dataframe to make it suitable for presentation on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log Regression</th>\n",
       "      <th>KNN</th>\n",
       "      <th>BernoulliNB</th>\n",
       "      <th>MultinomialNB</th>\n",
       "      <th>SVM</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1 Score(toxic)</th>\n",
       "      <td>0.861234</td>\n",
       "      <td>0.185120</td>\n",
       "      <td>0.776521</td>\n",
       "      <td>0.874958</td>\n",
       "      <td>0.876133</td>\n",
       "      <td>0.838055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score(severe_toxic)</th>\n",
       "      <td>0.927879</td>\n",
       "      <td>0.857416</td>\n",
       "      <td>0.803707</td>\n",
       "      <td>0.936170</td>\n",
       "      <td>0.926004</td>\n",
       "      <td>0.934874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score(obscene)</th>\n",
       "      <td>0.908655</td>\n",
       "      <td>0.519056</td>\n",
       "      <td>0.787830</td>\n",
       "      <td>0.901463</td>\n",
       "      <td>0.921378</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score(insult)</th>\n",
       "      <td>0.896599</td>\n",
       "      <td>0.257992</td>\n",
       "      <td>0.783762</td>\n",
       "      <td>0.897411</td>\n",
       "      <td>0.902619</td>\n",
       "      <td>0.883993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score(threat)</th>\n",
       "      <td>0.628821</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.504762</td>\n",
       "      <td>0.786765</td>\n",
       "      <td>0.795539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1 Score(identity_hate)</th>\n",
       "      <td>0.699029</td>\n",
       "      <td>0.230159</td>\n",
       "      <td>0.549206</td>\n",
       "      <td>0.485857</td>\n",
       "      <td>0.797516</td>\n",
       "      <td>0.768448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Log Regression       KNN  BernoulliNB  MultinomialNB  \\\n",
       "F1 Score(toxic)                0.861234  0.185120     0.776521       0.874958   \n",
       "F1 Score(severe_toxic)         0.927879  0.857416     0.803707       0.936170   \n",
       "F1 Score(obscene)              0.908655  0.519056     0.787830       0.901463   \n",
       "F1 Score(insult)               0.896599  0.257992     0.783762       0.897411   \n",
       "F1 Score(threat)               0.628821  0.720000     0.311828       0.504762   \n",
       "F1 Score(identity_hate)        0.699029  0.230159     0.549206       0.485857   \n",
       "\n",
       "                              SVM  Random Forest  \n",
       "F1 Score(toxic)          0.876133       0.838055  \n",
       "F1 Score(severe_toxic)   0.926004       0.934874  \n",
       "F1 Score(obscene)        0.921378       0.909091  \n",
       "F1 Score(insult)         0.902619       0.883993  \n",
       "F1 Score(threat)         0.786765       0.795539  \n",
       "F1 Score(identity_hate)  0.797516       0.768448  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_all_trp = f1_all.transpose()\n",
    "f1_all_trp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LinearSVC and Random Forest models perform best (purple and brown lines seem to be the highest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test if our code actually works. Probability of the comment falling in various categories should be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0005c987bdfc9d4b</td>\n",
       "      <td>hey    what is it       talk   what is it    a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0007e25b2121310b</td>\n",
       "      <td>bye    don t look  come or think of comming ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>001810bf8c45bf5f</td>\n",
       "      <td>you are gay or antisemmitian    archangel whit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>00190820581d90ce</td>\n",
       "      <td>fuck your filthy mother in the ass  dry</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  toxic\n",
       "6   0002bcb3da6cb337       cocksucker before you piss around on my work      1\n",
       "12  0005c987bdfc9d4b  hey    what is it       talk   what is it    a...      1\n",
       "16  0007e25b2121310b  bye    don t look  come or think of comming ba...      1\n",
       "42  001810bf8c45bf5f  you are gay or antisemmitian    archangel whit...      1\n",
       "43  00190820581d90ce           fuck your filthy mother in the ass  dry       1"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tox_done.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_tox_done.comment_text\n",
    "y = data_tox_done['toxic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initiate a Tfidf vectorizer\n",
    "tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "X_train_fit = tfv.fit_transform(X_train)  # Convert the X data into a document term matrix dataframe\n",
    "X_test_fit = tfv.transform(X_test)  # Converts the X_test comments into Vectorized format\n",
    "\n",
    "randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train our SVM model with the X training data converted into Count Vectorized format with the Y training data\n",
    "randomforest.fit(X_train_fit, y_train)\n",
    "randomforest.predict(X_test_fit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Prediction\n",
    "comment1 = ['You piece of shit']\n",
    "comment2 = ['What is up garden apple doing']\n",
    "\n",
    "comment1_vect = tfv.transform(comment1)\n",
    "randomforest.predict_proba(comment1_vect)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.16036935])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment2_vect = tfv.transform(comment2)\n",
    "randomforest.predict_proba(comment2_vect)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest has assigned a probability of 1 that comment 1 ('you piece of shit') is toxic. Comment 2 has only a low toxicity probability of 0.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling trained RandomForest models for all categories. \n",
    "\n",
    "#### We choose Random Forest instead of LinearSVC although the latter performs well, as RDF has predict_proba function and LinearSVC does not. We need to output a probability score for each comment, remember?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to pickle not only the TF-IDF vectorizer object, but also the RDF model trained on the related vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "NOTE: This step has already been done for you. The pickled files are included in the github. Just for further reference.\n",
    "You do NOT have to run this cell.\n",
    "'''\n",
    "\n",
    "def pickle_model(df, label):\n",
    "    \n",
    "    X = df.comment_text\n",
    "    y = df[label]\n",
    "\n",
    "    # Initiate a Tfidf vectorizer\n",
    "    tfv = TfidfVectorizer(ngram_range=(1,1), stop_words='english')\n",
    "    \n",
    "    # Convert the X data into a document term matrix dataframe\n",
    "    X_vect = tfv.fit_transform(X)  \n",
    "    \n",
    "    # saves the column labels (ie. the vocabulary)\n",
    "    # wb means Writing to the file in Binary mode, written in byte objects\n",
    "    with open(r\"{}.pkl\".format(label + '_vect'), \"wb\") as f:   \n",
    "        pickle.dump(tfv, f)   \n",
    "        \n",
    "    randomforest = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    randomforest.fit(X_vect, y)\n",
    "\n",
    "    # Create a new pickle file based on random forest\n",
    "    with open(r\"{}.pkl\".format(label + '_model'), \"wb\") as f:  \n",
    "        pickle.dump(randomforest, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a loop to create pickle files all at one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "OPTIONAL STEP. Pickled file already available on Github.\n",
    "\n",
    "datalist = [data_tox_done, data_sev_done, data_obs_done, data_ins_done, data_thr_done, data_ide_done]\n",
    "label = ['toxic', 'severe_toxic', 'obscene', 'insult', 'threat', 'identity_hate']\n",
    "\n",
    "for i,j in zip(datalist,label):\n",
    "    pickle_model(i, j)\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toxic",
   "language": "python",
   "name": "toxic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
